{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c30a1c3f-778c-45cc-aaff-12840f6e0353",
   "metadata": {},
   "source": [
    "##  Efficiently consuming key points of lengthy documents through concise audio summaries.\n",
    "Description: Develop a proof-of-concept solution to generate concise audio summaries of given documents. The objective is to help professionals quickly grasp essential details and implications. Break down the task into smaller steps and outline the actions needed to achieve this. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f617ba6-ece5-4ac3-baaa-2c1adefa6d69",
   "metadata": {},
   "source": [
    "-\tImplement TTS – Text-to-Speech for below summary into using python.\n",
    "-\tResearch different TTS - (Text-to-Speech) technologies and implement all your researched technologies for given summary.\n",
    "-\tTTS conversion should be contextual and concise enough to understand easily, instead of just word to word conversion.\n",
    "-\tPresent your findings for implemented technologies for evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9418ef81-fbd2-4e1b-b2bb-8c770d898a63",
   "metadata": {},
   "source": [
    "# Working on Text To Speech "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7a0e97",
   "metadata": {},
   "source": [
    "#### Experiment Texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "229e84ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "introduction = '''\n",
    "Introduction: This text discusses a judgment from the Supreme Court of India regarding a complaint filed under Section 138 of the Negotiable Instruments Act. The case involves a dispute over a cheque issued by the respondent, which was returned due to insufficient funds. The Trial Court initially dismissed the complaint, but the Supreme Court upheld it, finding that the cheque was indeed issued by the respondent.\n",
    "'''\n",
    "key_points= '''Key Points: \n",
    "1.The complaint was dismissed initially due to contradictions in the evidence regarding the number of apple cartons and the amount owed.\n",
    "2.The High Court established that a cheque carries a presumption of consideration unless proven otherwise.\n",
    "3.The burden of proof is on the accused to rebut the presumption of consideration by providing evidence or circumstances to show that no debt existed.\n",
    "4.The court discusses the presumption of debt or liability under Section 139 of the Act and states that it may fail if the accused raises a probable defense.\n",
    "5.The court emphasizes that the presumption under Section 139 is a device to prevent undue delay in litigation and that dishonoring a check is largely a civil wrong.\n",
    "6.The respondent in this case failed to provide any evidence to rebut the presumption of consideration in issuing the cheque.\n",
    "7.The courts below were criticized for dismissing the complaint based on discrepancies in the determination of the amount due.\n",
    "8.The respondent is held guilty of dishonoring the cheque and is ordered to pay a fine and costs.\n",
    "'''\n",
    "conclution='''\n",
    "Conclusion:In conclusion, the Supreme Court of India upheld a complaint filed under Section 138 of the Negotiable Instruments Act. The court found that the cheque was issued by the respondent and criticized the lower courts for dismissing the complaint based on discrepancies in the evidence. The court emphasized the presumption of consideration under Section 139 and held the respondent guilty of dishonoring the cheque. The respondent was ordered to pay a fine and costs.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b334b0b8-c7ae-4223-b09b-479636b86f84",
   "metadata": {},
   "source": [
    "### Coqui TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed8e5bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Name format: type/language/dataset/model\n",
      " 1: tts_models/multilingual/multi-dataset/xtts_v2 [already downloaded]\n",
      " 2: tts_models/multilingual/multi-dataset/xtts_v1.1\n",
      " 3: tts_models/multilingual/multi-dataset/your_tts\n",
      " 4: tts_models/multilingual/multi-dataset/bark\n",
      " 5: tts_models/bg/cv/vits\n",
      " 6: tts_models/cs/cv/vits\n",
      " 7: tts_models/da/cv/vits\n",
      " 8: tts_models/et/cv/vits\n",
      " 9: tts_models/ga/cv/vits\n",
      " 10: tts_models/en/ek1/tacotron2\n",
      " 11: tts_models/en/ljspeech/tacotron2-DDC\n",
      " 12: tts_models/en/ljspeech/tacotron2-DDC_ph\n",
      " 13: tts_models/en/ljspeech/glow-tts\n",
      " 14: tts_models/en/ljspeech/speedy-speech\n",
      " 15: tts_models/en/ljspeech/tacotron2-DCA\n",
      " 16: tts_models/en/ljspeech/vits\n",
      " 17: tts_models/en/ljspeech/vits--neon\n",
      " 18: tts_models/en/ljspeech/fast_pitch\n",
      " 19: tts_models/en/ljspeech/overflow\n",
      " 20: tts_models/en/ljspeech/neural_hmm\n",
      " 21: tts_models/en/vctk/vits\n",
      " 22: tts_models/en/vctk/fast_pitch\n",
      " 23: tts_models/en/sam/tacotron-DDC\n",
      " 24: tts_models/en/blizzard2013/capacitron-t2-c50\n",
      " 25: tts_models/en/blizzard2013/capacitron-t2-c150_v2\n",
      " 26: tts_models/en/multi-dataset/tortoise-v2\n",
      " 27: tts_models/en/jenny/jenny\n",
      " 28: tts_models/es/mai/tacotron2-DDC\n",
      " 29: tts_models/es/css10/vits\n",
      " 30: tts_models/fr/mai/tacotron2-DDC\n",
      " 31: tts_models/fr/css10/vits\n",
      " 32: tts_models/uk/mai/glow-tts\n",
      " 33: tts_models/uk/mai/vits\n",
      " 34: tts_models/zh-CN/baker/tacotron2-DDC-GST\n",
      " 35: tts_models/nl/mai/tacotron2-DDC\n",
      " 36: tts_models/nl/css10/vits\n",
      " 37: tts_models/de/thorsten/tacotron2-DCA\n",
      " 38: tts_models/de/thorsten/vits\n",
      " 39: tts_models/de/thorsten/tacotron2-DDC [already downloaded]\n",
      " 40: tts_models/de/css10/vits-neon\n",
      " 41: tts_models/ja/kokoro/tacotron2-DDC\n",
      " 42: tts_models/tr/common-voice/glow-tts\n",
      " 43: tts_models/it/mai_female/glow-tts\n",
      " 44: tts_models/it/mai_female/vits\n",
      " 45: tts_models/it/mai_male/glow-tts\n",
      " 46: tts_models/it/mai_male/vits\n",
      " 47: tts_models/ewe/openbible/vits\n",
      " 48: tts_models/hau/openbible/vits\n",
      " 49: tts_models/lin/openbible/vits\n",
      " 50: tts_models/tw_akuapem/openbible/vits\n",
      " 51: tts_models/tw_asante/openbible/vits\n",
      " 52: tts_models/yor/openbible/vits\n",
      " 53: tts_models/hu/css10/vits\n",
      " 54: tts_models/el/cv/vits\n",
      " 55: tts_models/fi/css10/vits\n",
      " 56: tts_models/hr/cv/vits\n",
      " 57: tts_models/lt/cv/vits\n",
      " 58: tts_models/lv/cv/vits\n",
      " 59: tts_models/mt/cv/vits\n",
      " 60: tts_models/pl/mai_female/vits\n",
      " 61: tts_models/pt/cv/vits\n",
      " 62: tts_models/ro/cv/vits\n",
      " 63: tts_models/sk/cv/vits\n",
      " 64: tts_models/sl/cv/vits\n",
      " 65: tts_models/sv/cv/vits\n",
      " 66: tts_models/ca/custom/vits\n",
      " 67: tts_models/fa/custom/glow-tts\n",
      " 68: tts_models/bn/custom/vits-male\n",
      " 69: tts_models/bn/custom/vits-female\n",
      " 70: tts_models/be/common-voice/glow-tts\n",
      "\n",
      " Name format: type/language/dataset/model\n",
      " 1: vocoder_models/universal/libri-tts/wavegrad\n",
      " 2: vocoder_models/universal/libri-tts/fullband-melgan\n",
      " 3: vocoder_models/en/ek1/wavegrad\n",
      " 4: vocoder_models/en/ljspeech/multiband-melgan\n",
      " 5: vocoder_models/en/ljspeech/hifigan_v2\n",
      " 6: vocoder_models/en/ljspeech/univnet\n",
      " 7: vocoder_models/en/blizzard2013/hifigan_v2\n",
      " 8: vocoder_models/en/vctk/hifigan_v2\n",
      " 9: vocoder_models/en/sam/hifigan_v2\n",
      " 10: vocoder_models/nl/mai/parallel-wavegan\n",
      " 11: vocoder_models/de/thorsten/wavegrad\n",
      " 12: vocoder_models/de/thorsten/fullband-melgan\n",
      " 13: vocoder_models/de/thorsten/hifigan_v1 [already downloaded]\n",
      " 14: vocoder_models/ja/kokoro/hifigan_v1\n",
      " 15: vocoder_models/uk/mai/multiband-melgan\n",
      " 16: vocoder_models/tr/common-voice/hifigan\n",
      " 17: vocoder_models/be/common-voice/hifigan\n",
      "\n",
      " Name format: type/language/dataset/model\n",
      " 1: voice_conversion_models/multilingual/vctk/freevc24\n"
     ]
    }
   ],
   "source": [
    "!tts --list_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adc092a3-7a89-4352-a505-6e8494f462bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/de/thorsten/tacotron2-DDC is already downloaded.\n",
      " > vocoder_models/de/thorsten/hifigan_v1 is already downloaded.\n",
      " > Using model: tacotron2\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:50.0\n",
      " | > mel_fmax:None\n",
      " | > pitch_fmin:0.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": " [!] No espeak backend found. Install espeak-ng or espeak to your system.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# # generate speech by cloning a voice using default settings\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#                 # speaker_wav=\"/path/to/target/speaker.wav\",\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#                 language=\"en\")\u001b[39;00m\n\u001b[1;32m     12\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 14\u001b[0m tts \u001b[38;5;241m=\u001b[39m \u001b[43mTTS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtts_models/de/thorsten/tacotron2-DDC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Run TTS\u001b[39;00m\n\u001b[1;32m     17\u001b[0m tts\u001b[38;5;241m.\u001b[39mtts_to_file(text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIch bin eine Testnachricht.\u001b[39m\u001b[38;5;124m\"\u001b[39m, file_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout.wav\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/workspaces/Task_2_audio_summary_of_keypoints/.tts/lib/python3.10/site-packages/TTS/api.py:74\u001b[0m, in \u001b[0;36mTTS.__init__\u001b[0;34m(self, model_name, model_path, config_path, vocoder_path, vocoder_config_path, progress_bar, gpu)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model_name) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtts_models\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_name:\n\u001b[0;32m---> 74\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_tts_model_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvoice_conversion_models\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_name:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_vc_model_by_name(model_name, gpu)\n",
      "File \u001b[0;32m/workspaces/Task_2_audio_summary_of_keypoints/.tts/lib/python3.10/site-packages/TTS/api.py:177\u001b[0m, in \u001b[0;36mTTS.load_tts_model_by_name\u001b[0;34m(self, model_name, gpu)\u001b[0m\n\u001b[1;32m    171\u001b[0m model_path, config_path, vocoder_path, vocoder_config_path, model_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_model_by_name(\n\u001b[1;32m    172\u001b[0m     model_name\n\u001b[1;32m    173\u001b[0m )\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# init synthesizer\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# None values are fetch from the model\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msynthesizer \u001b[38;5;241m=\u001b[39m \u001b[43mSynthesizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtts_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtts_config_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtts_speakers_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtts_languages_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocoder_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocoder_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocoder_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocoder_config_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/Task_2_audio_summary_of_keypoints/.tts/lib/python3.10/site-packages/TTS/utils/synthesizer.py:93\u001b[0m, in \u001b[0;36mSynthesizer.__init__\u001b[0;34m(self, tts_checkpoint, tts_config_path, tts_speakers_file, tts_languages_file, vocoder_checkpoint, vocoder_config, encoder_checkpoint, encoder_config, vc_checkpoint, vc_config, model_dir, voice_dir, use_cuda)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA is not availabe on this machine.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tts_checkpoint:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_tts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtts_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtts_config_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cuda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_sample_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtts_config\u001b[38;5;241m.\u001b[39maudio[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vocoder_checkpoint:\n",
      "File \u001b[0;32m/workspaces/Task_2_audio_summary_of_keypoints/.tts/lib/python3.10/site-packages/TTS/utils/synthesizer.py:187\u001b[0m, in \u001b[0;36mSynthesizer._load_tts\u001b[0;34m(self, tts_checkpoint, tts_config_path, use_cuda)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtts_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_phonemes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtts_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphonemizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhonemizer is not defined in the TTS config.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtts_model \u001b[38;5;241m=\u001b[39m \u001b[43msetup_tts_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtts_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_checkpoint:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_speaker_encoder_paths_from_tts_config()\n",
      "File \u001b[0;32m/workspaces/Task_2_audio_summary_of_keypoints/.tts/lib/python3.10/site-packages/TTS/tts/models/__init__.py:13\u001b[0m, in \u001b[0;36msetup_model\u001b[0;34m(config, samples)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     MyModel \u001b[38;5;241m=\u001b[39m find_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTTS.tts.models\u001b[39m\u001b[38;5;124m\"\u001b[39m, config\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlower())\n\u001b[0;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMyModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/workspaces/Task_2_audio_summary_of_keypoints/.tts/lib/python3.10/site-packages/TTS/tts/models/tacotron2.py:431\u001b[0m, in \u001b[0;36mTacotron2.init_from_config\u001b[0;34m(config, samples)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mTTS\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AudioProcessor\n\u001b[1;32m    430\u001b[0m ap \u001b[38;5;241m=\u001b[39m AudioProcessor\u001b[38;5;241m.\u001b[39minit_from_config(config)\n\u001b[0;32m--> 431\u001b[0m tokenizer, new_config \u001b[38;5;241m=\u001b[39m \u001b[43mTTSTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m speaker_manager \u001b[38;5;241m=\u001b[39m SpeakerManager\u001b[38;5;241m.\u001b[39minit_from_config(new_config, samples)\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Tacotron2(new_config, ap, tokenizer, speaker_manager)\n",
      "File \u001b[0;32m/workspaces/Task_2_audio_summary_of_keypoints/.tts/lib/python3.10/site-packages/TTS/tts/utils/text/tokenizer.py:198\u001b[0m, in \u001b[0;36mTTSTokenizer.init_from_config\u001b[0;34m(config, characters)\u001b[0m\n\u001b[1;32m    196\u001b[0m phonemizer_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m: config\u001b[38;5;241m.\u001b[39mphoneme_language}\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphonemizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config \u001b[38;5;129;01mand\u001b[39;00m config\u001b[38;5;241m.\u001b[39mphonemizer:\n\u001b[0;32m--> 198\u001b[0m     phonemizer \u001b[38;5;241m=\u001b[39m \u001b[43mget_phonemizer_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphonemizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mphonemizer_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/workspaces/Task_2_audio_summary_of_keypoints/.tts/lib/python3.10/site-packages/TTS/tts/utils/text/phonemizers/__init__.py:60\u001b[0m, in \u001b[0;36mget_phonemizer_by_name\u001b[0;34m(name, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initiate a phonemizer by name\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m        Extra keyword arguments that should be passed to the phonemizer.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mespeak\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mESpeak\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgruut\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Gruut(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/workspaces/Task_2_audio_summary_of_keypoints/.tts/lib/python3.10/site-packages/TTS/tts/utils/text/phonemizers/espeak_wrapper.py:114\u001b[0m, in \u001b[0;36mESpeak.__init__\u001b[0;34m(self, language, backend, punctuations, keep_puncs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, language: \u001b[38;5;28mstr\u001b[39m, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, punctuations\u001b[38;5;241m=\u001b[39mPunctuation\u001b[38;5;241m.\u001b[39mdefault_puncs(), keep_puncs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ESPEAK_LIB \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m [!] No espeak backend found. Install espeak-ng or espeak to your system.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ESPEAK_LIB\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# band-aid for backwards compatibility\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m:  [!] No espeak backend found. Install espeak-ng or espeak to your system."
     ]
    }
   ],
   "source": [
    "from TTS.api import TTS\n",
    "import torch\n",
    "# tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\")\n",
    "\n",
    "# # generate speech by cloning a voice using default settings\n",
    "# tts.tts_to_file(text=\"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n",
    "#                 file_path=\"output.wav\",\n",
    "#                 # speaker_wav=\"/path/to/target/speaker.wav\",\n",
    "#                 language=\"en\")\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tts = TTS(model_name=\"tts_models/de/thorsten/tacotron2-DDC\", progress_bar=False).to(device)\n",
    "\n",
    "# Run TTS\n",
    "tts.tts_to_file(text=\"Ich bin eine Testnachricht.\", file_path='out.wav')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106ef3e9-5dc4-47af-8bca-f452a4f75955",
   "metadata": {},
   "source": [
    "### TensorFlow TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0593d650",
   "metadata": {},
   "outputs": [],
   "source": [
    "git clone https://github.com/TensorSpeech/TensorFlowTTS.git\n",
    "cd TensorFlowTTS\n",
    "pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a9926af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'tensorspeech/tts-tacotron2-ljspeech-en'\n",
    "# model_name = 'tensorspeech/tts-melgan-ljspeech-en'\n",
    "# model_name = 'tensorspeech/tts-mb_melgan-ljspeech-en'\n",
    "# model_name = 'tensorspeech/tts-fastspeech2-ljspeech-en'\n",
    "# model_name = 'tensorspeech/tts-fastspeech-ljspeech-en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e98e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import yaml\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_tts.inference import TFAutoModel\n",
    "from tensorflow_tts.inference import AutoProcessor\n",
    "\n",
    "# initialize fastspeech2 model.\n",
    "fastspeech2 = TFAutoModel.from_pretrained(\"tensorspeech/tts-fastspeech2-ljspeech-en\")\n",
    "\n",
    "\n",
    "# initialize mb_melgan model\n",
    "mb_melgan = TFAutoModel.from_pretrained(\"tensorspeech/tts-mb_melgan-ljspeech-en\")\n",
    "\n",
    "\n",
    "# inference\n",
    "processor = AutoProcessor.from_pretrained(\"tensorspeech/tts-fastspeech2-ljspeech-en\")\n",
    "\n",
    "input_ids = processor.text_to_sequence(\"Recent research at Harvard has shown meditating for as little as 8 weeks, can actually increase the grey matter in the parts of the brain responsible for emotional regulation, and learning.\")\n",
    "# fastspeech inference\n",
    "\n",
    "mel_before, mel_after, duration_outputs, _, _ = fastspeech2.inference(\n",
    "    input_ids=tf.expand_dims(tf.convert_to_tensor(input_ids, dtype=tf.int32), 0),\n",
    "    speaker_ids=tf.convert_to_tensor([0], dtype=tf.int32),\n",
    "    speed_ratios=tf.convert_to_tensor([1.0], dtype=tf.float32),\n",
    "    f0_ratios =tf.convert_to_tensor([1.0], dtype=tf.float32),\n",
    "    energy_ratios =tf.convert_to_tensor([1.0], dtype=tf.float32),\n",
    ")\n",
    "\n",
    "# melgan inference\n",
    "audio_before = mb_melgan.inference(mel_before)[0, :, 0]\n",
    "audio_after = mb_melgan.inference(mel_after)[0, :, 0]\n",
    "\n",
    "# save to file\n",
    "sf.write('./audio_before.wav', audio_before, 22050, \"PCM_16\")\n",
    "sf.write('./audio_after.wav', audio_after, 22050, \"PCM_16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbf29e3-d938-4758-95ce-257783271234",
   "metadata": {},
   "source": [
    "### ESPNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50f6281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a737cf30-8342-4e61-a2f1-c690da35e067",
   "metadata": {},
   "source": [
    "### Hugging face Opensource models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48399290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95c9514685142179bd9cb053b71a6c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c1834f",
   "metadata": {},
   "source": [
    "##### Approach 1\n",
    "**Model** : \"microsoft/speecht5_tts\"            <br>\n",
    "**Datasets** : \"Matthijs/cmu-arctic-xvectors\"   <br>\n",
    "Resource : Hugging Face \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d950ac7-c5ae-4027-b435-df5fd5658dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "import torch\n",
    "synthesiser = pipeline(\"text-to-speech\", \"microsoft/speecht5_tts\")\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "speaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
    "speech = synthesiser(key_points,forward_params={\"speaker_embeddings\": speaker_embedding})\n",
    "sf.write(\"speech.wav\", speech[\"audio\"], samplerate=speech[\"sampling_rate\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a1bfaff-7727-454c-9895-e4dbfa827235",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GenerationConfig' object has no attribute 'sample_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 23\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# filename=f\"./speechT5_outputs/speech{i}.wav\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# sf.write(filename, speech.numpy(), samplerate=16000)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Audio\n\u001b[1;32m---> 23\u001b[0m sampling_rate \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_rate\u001b[49m\n\u001b[0;32m     24\u001b[0m Audio(speech\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39msqueeze(), rate\u001b[38;5;241m=\u001b[39msampling_rate)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GenerationConfig' object has no attribute 'sample_rate'"
     ]
    }
   ],
   "source": [
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "from datasets import load_dataset, Audio\n",
    "import torch\n",
    "import soundfile as sf\n",
    "from datasets import load_dataset\n",
    "\n",
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "inputs = processor(text=introduction, return_tensors=\"pt\")\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "\n",
    "speaker_embeddings = torch.tensor(embeddings_dataset[7000+6][\"xvector\"]).unsqueeze(0)\n",
    "\n",
    "speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n",
    "# filename=f\"./speechT5_outputs/speech{i}.wav\"\n",
    "# sf.write(filename, speech.numpy(), samplerate=16000)\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "sampling_rate = model.generation_config.sample_rate\n",
    "Audio(speech.cpu().numpy().squeeze(), rate=sampling_rate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6772bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7282e785",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m     13\u001b[0m     speaker_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(embeddings_dataset[\u001b[38;5;241m647\u001b[39m\u001b[38;5;241m*\u001b[39mi][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxvector\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m     speech \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_speech\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeaker_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocoder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./speechT5_outputs/speech\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m     sf\u001b[38;5;241m.\u001b[39mwrite(filename, speech\u001b[38;5;241m.\u001b[39mnumpy(), samplerate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m)\n",
      "File \u001b[1;32md:\\AritraRanjanChowdhury\\GEN_AI\\Task_2_audio_summary_of_keypoints\\.tts\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\AritraRanjanChowdhury\\GEN_AI\\Task_2_audio_summary_of_keypoints\\.tts\\Lib\\site-packages\\transformers\\models\\speecht5\\modeling_speecht5.py:2930\u001b[0m, in \u001b[0;36mSpeechT5ForTextToSpeech.generate_speech\u001b[1;34m(self, input_ids, speaker_embeddings, attention_mask, threshold, minlenratio, maxlenratio, vocoder, output_cross_attentions, return_output_lengths)\u001b[0m\n\u001b[0;32m   2925\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2926\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2927\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe first dimension of speaker_embeddings must be either 1 or the same as batch size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2928\u001b[0m             )\n\u001b[1;32m-> 2930\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_generate_speech\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2931\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2932\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2933\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeaker_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mminlenratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaxlenratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2939\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_cross_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_output_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2941\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\AritraRanjanChowdhury\\GEN_AI\\Task_2_audio_summary_of_keypoints\\.tts\\Lib\\site-packages\\transformers\\models\\speecht5\\modeling_speecht5.py:2516\u001b[0m, in \u001b[0;36m_generate_speech\u001b[1;34m(model, input_values, speaker_embeddings, attention_mask, threshold, minlenratio, maxlenratio, vocoder, output_cross_attentions, return_output_lengths)\u001b[0m\n\u001b[0;32m   2513\u001b[0m idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2515\u001b[0m \u001b[38;5;66;03m# Run the decoder prenet on the entire output sequence.\u001b[39;00m\n\u001b[1;32m-> 2516\u001b[0m decoder_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspeecht5\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprenet\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeaker_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2517\u001b[0m \u001b[38;5;66;03m# Run the decoder layers on the last element of the prenet output.\u001b[39;00m\n\u001b[0;32m   2518\u001b[0m decoder_out \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mspeecht5\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mwrapped_decoder(\n\u001b[0;32m   2519\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mdecoder_hidden_states[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m   2520\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2526\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   2527\u001b[0m )\n",
      "File \u001b[1;32md:\\AritraRanjanChowdhury\\GEN_AI\\Task_2_audio_summary_of_keypoints\\.tts\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\AritraRanjanChowdhury\\GEN_AI\\Task_2_audio_summary_of_keypoints\\.tts\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\AritraRanjanChowdhury\\GEN_AI\\Task_2_audio_summary_of_keypoints\\.tts\\Lib\\site-packages\\transformers\\models\\speecht5\\modeling_speecht5.py:688\u001b[0m, in \u001b[0;36mSpeechT5SpeechDecoderPrenet.forward\u001b[1;34m(self, input_values, speaker_embeddings)\u001b[0m\n\u001b[0;32m    686\u001b[0m inputs_embeds \u001b[38;5;241m=\u001b[39m input_values\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 688\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    689\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consistent_dropout(inputs_embeds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mspeech_decoder_prenet_dropout)\n\u001b[0;32m    691\u001b[0m inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer(inputs_embeds)\n",
      "File \u001b[1;32md:\\AritraRanjanChowdhury\\GEN_AI\\Task_2_audio_summary_of_keypoints\\.tts\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\AritraRanjanChowdhury\\GEN_AI\\Task_2_audio_summary_of_keypoints\\.tts\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\AritraRanjanChowdhury\\GEN_AI\\Task_2_audio_summary_of_keypoints\\.tts\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import soundfile as sf\n",
    "from datasets import load_dataset\n",
    "\n",
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "inputs = processor(text=introduction, return_tensors=\"pt\")\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "for i in range(100):\n",
    "    speaker_embeddings = torch.tensor(embeddings_dataset[647*i][\"xvector\"]).unsqueeze(0)\n",
    "\n",
    "    speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n",
    "    filename=f\"./speechT5_outputs/speech{i}.wav\"\n",
    "    sf.write(filename, speech.numpy(), samplerate=16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52c40fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.speecht5.processing_speecht5.SpeechT5Processor'>\n",
      "<class 'transformers.models.speecht5.modeling_speecht5.SpeechT5ForTextToSpeech'>\n",
      "<class 'transformers.models.speecht5.modeling_speecht5.SpeechT5HifiGan'>\n",
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(processor))\n",
    "print(type(model))\n",
    "print(type(vocoder))\n",
    "print(type(inputs))\n",
    "print(type(embeddings_dataset))\n",
    "print(type(speaker_embeddings))\n",
    "print(type(speech))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b508cf02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7931"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d306d35",
   "metadata": {},
   "source": [
    "#####  Approach 2\n",
    "**Model** : \"suno/bark\"    <br>\n",
    "Resource : Hugging Face "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "890f8de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:10000 for open-end generation.\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "ushort format requires 0 <= number <= 65535",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m synthesiser \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-to-speech\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuno/bark\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m speech \u001b[38;5;241m=\u001b[39m synthesiser(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, my dog is cooler than you!\u001b[39m\u001b[38;5;124m\"\u001b[39m, forward_params\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_sample\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m})\n\u001b[1;32m----> 8\u001b[0m \u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwavfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbark_out.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeech\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msampling_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeech\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\AritraRanjanChowdhury\\GEN_AI\\Task_2_audio_summary_of_keypoints\\.tts\\Lib\\site-packages\\scipy\\io\\wavfile.py:796\u001b[0m, in \u001b[0;36mwrite\u001b[1;34m(filename, rate, data)\u001b[0m\n\u001b[0;32m    793\u001b[0m bytes_per_second \u001b[38;5;241m=\u001b[39m fs\u001b[38;5;241m*\u001b[39m(bit_depth \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m8\u001b[39m)\u001b[38;5;241m*\u001b[39mchannels\n\u001b[0;32m    794\u001b[0m block_align \u001b[38;5;241m=\u001b[39m channels \u001b[38;5;241m*\u001b[39m (bit_depth \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m--> 796\u001b[0m fmt_chunk_data \u001b[38;5;241m=\u001b[39m \u001b[43mstruct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<HHIIHH\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_tag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mbytes_per_second\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_align\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbit_depth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (dkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;66;03m# add cbSize field for non-PCM files\u001b[39;00m\n\u001b[0;32m    800\u001b[0m     fmt_chunk_data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\x00\u001b[39;00m\u001b[38;5;130;01m\\x00\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31merror\u001b[0m: ushort format requires 0 <= number <= 65535"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import scipy\n",
    "\n",
    "synthesiser = pipeline(\"text-to-speech\", \"suno/bark\")\n",
    "\n",
    "speech = synthesiser(\"Hello, my dog is cooler than you!\", forward_params={\"do_sample\": True})\n",
    "\n",
    "scipy.io.wavfile.write(\"bark_out.wav\", rate=speech[\"sampling_rate\"], data=speech[\"audio\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51351597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "speaker_embeddings_path.json: 100%|██████████| 61.1k/61.1k [00:00<00:00, 351kB/s]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:10000 for open-end generation.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BarkConfig' object has no attribute 'sample_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m speech_values \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m sampling_rate \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_rate\u001b[49m\n\u001b[0;32m     15\u001b[0m scipy\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mwavfile\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbark_out.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m, rate\u001b[38;5;241m=\u001b[39msampling_rate, data\u001b[38;5;241m=\u001b[39mspeech_values\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39msqueeze())\n",
      "File \u001b[1;32md:\\AritraRanjanChowdhury\\GEN_AI\\Task_2_audio_summary_of_keypoints\\.tts\\Lib\\site-packages\\transformers\\configuration_utils.py:265\u001b[0m, in \u001b[0;36mPretrainedConfig.__getattribute__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    264\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m)[key]\n\u001b[1;32m--> 265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BarkConfig' object has no attribute 'sample_rate'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"suno/bark\")\n",
    "model = AutoModel.from_pretrained(\"suno/bark\")\n",
    "\n",
    "inputs = processor(\n",
    "    text=[\"Hello, my name is Suno. And, uh — and I like pizza. [laughs] But I also have other interests such as playing tic tac toe.\"],\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "speech_values = model.generate(**inputs, do_sample=True)\n",
    "import scipy\n",
    "\n",
    "sampling_rate = model.config.sample_rate\n",
    "scipy.io.wavfile.write(\"bark_out.wav\", rate=sampling_rate, data=speech_values.cpu().numpy().squeeze())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62e8c59",
   "metadata": {},
   "source": [
    "### Google Text To Speech (gTTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a8975fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS \n",
    "tts = gTTS(introduction)\n",
    "tts.save('g_introduction.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f70d97-9f6e-402a-998e-2aa9427215f4",
   "metadata": {},
   "source": [
    "### Amazon Polly Text to Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ae033e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b689eb71",
   "metadata": {},
   "source": [
    "### IBM Whatsonx Text to Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9b552f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed359cbb-e00c-4002-adfe-b2ee344c81b2",
   "metadata": {},
   "source": [
    "# Working With Summarization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f523e3f5-c077-4ceb-bbf1-3ebf56306f41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
